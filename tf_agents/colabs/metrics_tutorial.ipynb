{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LLddJfAgbtmi"
      },
      "source": [
        "##### Copyright 2018 The TF-Agents Authors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MimUC9NrYFaS"
      },
      "source": [
        "### Get Started\n",
        "\u003ctable class=\"tfo-notebook-buttons\" align=\"left\"\u003e\n",
        "  \u003ctd\u003e\n",
        "    \u003ca target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/agents/blob/master/tf_agents/colabs/metrics_tutorial.ipynb\"\u003e\u003cimg src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" /\u003eRun in Google Colab\u003c/a\u003e\n",
        "  \u003c/td\u003e\n",
        "  \u003ctd\u003e\n",
        "    \u003ca target=\"_blank\" href=\"https://github.com/tensorflow/agents/blob/master/tf_agents/colabs/metrics_tutorial.ipynb\"\u003e\u003cimg src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" /\u003eView source on GitHub\u003c/a\u003e\n",
        "  \u003c/td\u003e\n",
        "\u003c/table\u003e\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "zwQ2P62Ub4kD"
      },
      "outputs": [],
      "source": [
        "# Note: If you haven't installed tf-agents yet, run:\n",
        "!pip install tf-agents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "O7gLdUS6b2EG"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "3oCS94Z83Jo2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "nest = tf.contrib.framework.nest\n",
        "\n",
        "from tf_agents.environments import time_step as ts\n",
        "from tf_agents.environments import trajectory\n",
        "\n",
        "from tf_agents.metrics import py_metric\n",
        "from tf_agents.metrics import py_metrics\n",
        "from tf_agents.metrics import tf_metric\n",
        "from tf_agents.metrics import tf_metrics\n",
        "from tf_agents.metrics import batched_py_metric\n",
        "from tf_agents.metrics import tf_py_metric\n",
        "\n",
        "# Clear any leftover state from previous colabs run.\n",
        "# (This is not necessary for normal programs.)\n",
        "tf.reset_default_graph()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CcIob6rYqien"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "A metric is a scalar value that gets updated continuously during the training or evaluation of a reinforcement learning agent in an environemnt. Often times, a metric measures the performance of the agent (e.g., the average return over the last 10 episodes). Other times, metrics track auxiliary tracking information from the environment or training loop (e.g. the number of episodes the agent has seen so far).\n",
        "\n",
        "`StepMetric`s are calculated based on the [trajectory](https://github.com/tensorflow/agents/tree/master/tf_agents/environments/trajectory.py) which includes information such as the observation, action, reward, discount, etc. For example, one of the most common metrics used in RL is the return or sum of the rewards in an episode.\n",
        "\n",
        "Metrics can be implemented either in Python (see [PyMetric](https://github.com/tensorflow/agents/tree/master/tf_agents/metrics/py_metric.py)) or Tensorflow (see [TFStepMetric](github.com/tensorflow/agents/tree/master/metrics/tf_metric.py)). Usually, the easiest way to implement a metric is to write it in Python for a single environment.  This metric can then be wrapped in a BatchedPyMetric to automatically make it work in batched/parallel environments. Both regular/batched metrics can be used in Tensorflow by wrapping them in a TFPyMetric."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sHnm3D0CqxfH"
      },
      "source": [
        "# Python Metrics\n",
        "\n",
        "Python metrics are commonly used during evaluation and out of graph data collection.  The interface for Python metrics look like this:\n",
        "\n",
        "---\n",
        "```python\n",
        "class PyMetric(object):\n",
        "\n",
        "  @property\n",
        "  def name(self):\n",
        "    \"\"\"Name of the metric.\"\"\"\n",
        "    return self._name\n",
        "\n",
        "  def __call__(self, *args):\n",
        "    \"\"\"Processes the input to update the metric.\"\"\"\n",
        "    self.call(*args)\n",
        "\n",
        "  @abc.abstractmethod\n",
        "  def reset(self):\n",
        "    \"\"\"Resets internal variables used to compute the metric.\"\"\"\n",
        "\n",
        "  @abc.abstractmethod\n",
        "  def result(self):\n",
        "    \"\"\"Returns the current value of the metric.\"\"\"\n",
        "\n",
        "  @staticmethod\n",
        "  def aggregate(metrics):\n",
        "    \"\"\"Aggregates a list of metrics of this class.\"\"\"\n",
        "    return np.mean([metric.result() for metric in metrics])\n",
        "\n",
        "\n",
        "class PyStepMetric(PyMetric):\n",
        "\n",
        "  @abc.abstractmethod\n",
        "  def call(self, trajectory):\n",
        "    \"\"\"Processes the trajectory to update the metric.\"\"\"\n",
        "```\n",
        "---\n",
        "To create a new metric, child classes override the `call()` method to specify how the new metric is updated at every call, and the `result()` method to specify how results are finalized and returned. We will look at a couple of examples below. (For a discussion of `aggregate()`, please see [BatchedPyMetric](#scrollTo=P-6Y2QRFqxyP).)\n",
        "\n",
        "We will look at a couple of examples below.  For more, see `tf_agents/py_metrics.py`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vtxyakVI9mGu"
      },
      "source": [
        "## Example 1: AverageReturnMetric\n",
        "\n",
        "Average Return is the most common metric used in reinforcement learning. A return is defined as the sum of rewards received by an agent in an episode, and average return refers to averaging this return across multiple episodes.\n",
        "\n",
        "### Streaming Metric\n",
        "\n",
        "`AverageReturnMetric` is implemented by sub classing `StreamingMetric` which has a Deque buffer to keep track of the last (up to) K values of the metric. Calling `result()` on the a streaming metric returns the average of the values in the buffer. \n",
        "\n",
        "\n",
        "---\n",
        "```python\n",
        "class StreamingMetric(py_metric.PyStepMetric):\n",
        "\n",
        "  def reset(self):\n",
        "    self._buffer.clear()\n",
        "    self._reset()\n",
        "\n",
        "  def add_to_buffer(self, value):\n",
        "    \"\"\"Appends a new value to the buffer.\"\"\"\n",
        "    self._buffer.append(value)\n",
        "\n",
        "  def result(self):\n",
        "    \"\"\"Returns the value of this metric.\"\"\"\n",
        "    return self._buffer and np.mean(self._buffer) or 0.0\n",
        "```\n",
        "---\n",
        "\n",
        "Child classes of `StreamingMetric` must override the `call(trajectory)` method. \n",
        "\n",
        "### Average Return Metric\n",
        "\n",
        "The `AverageReturnMetric` keeps track of the sum of rewards in the current episode in a variable called self.episode_return. This is updated in every `call(trajectory)` and added to the buffer at the end of the episode. Trajectories at the boundary between two episodes have an invalid reward, so they are ignored.\n",
        "\n",
        "---\n",
        "```python\n",
        "class AverageReturnMetric(StreamingMetric):\n",
        "  \"\"\"Computes the average undiscounted reward.\"\"\"\n",
        "\n",
        "  def _reset(self):\n",
        "    \"\"\"Resets stat gathering variables.\"\"\"\n",
        "    self._episode_return = 0\n",
        "\n",
        "  def call(self, trajectory):\n",
        "    \"\"\"Processes the trajectory to update the metric.\"\"\"\n",
        "    if not trajectory.is_boundary():\n",
        "      self._episode_return += trajectory.reward\n",
        "    if trajectory.is_last():\n",
        "      self.add_to_buffer(self._episode_return)\n",
        "      self._episode_return = 0      \n",
        "```\n",
        "---\n",
        "\n",
        "The `AverageReturnMetric.result()` method returns the average of the returns saved in the buffer (implemented in the base class `StreamingMetric`).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hD3vRGaeIhP9"
      },
      "source": [
        "The AverageReturnMetric can be used as:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "LvVpxIODIgj9"
      },
      "outputs": [],
      "source": [
        "metric = py_metrics.AverageReturnMetric()\n",
        "\n",
        "# TODO(kbanoop): Make this more readable using kwargs\n",
        "metric(trajectory.boundary((), (), (), 0., 1.))\n",
        "metric(trajectory.first((), (), (), 1., 1.))\n",
        "metric(trajectory.mid((), (), (), 2., 1.))\n",
        "metric(trajectory.last((), (), (), 3., 0.))\n",
        "metric(trajectory.boundary((), (), (), 0., 1.))\n",
        "metric(trajectory.first((), (), (), 4., 1.))\n",
        "metric(trajectory.mid((), (), (), 5., 1.))\n",
        "metric(trajectory.last((), (), (), 6., 0.))\n",
        "\n",
        "print metric.result()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "P-6Y2QRFqxyP"
      },
      "source": [
        "## Example 2: BatchedPyMetric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kAI3QHhMFTDm"
      },
      "source": [
        "Certain environments like BatchedPyEnvironment and ParallelPyEnvironment manage multiple independent copies of an environment. Therefore observations, actions, rewards etc and therefore trajectories are batched, one for each sub environment.\n",
        "\n",
        "An easy way to make regular Python metrics work with such batches of `Trajectories` is to wrap them in a `BatchedPyMetric`.  Internally `BatchedPyMetric` creates a metric for each environment. Every time `BatchedPyMetric` is called with a batch of `Trajectories`,  it unbatches them into a list of individual `Trajectoy`'s and calls a different metric with each `Trajectory`.\n",
        "\n",
        "The BatchedPyMetric is implemented roughly as follows (For more details see `tf_agents/metrics/batched_py_metric.py`):\n",
        "\n",
        "---\n",
        "```python\n",
        "class BatchedPyMetric(py_metric.PyStepMetric):\n",
        "\n",
        "  def call(self, batched_trajectory):\n",
        "    trajectories = unstack(batched_trajectory)\n",
        "    for metric, trajectory in zip(self._metrics, trajectories):\n",
        "      metric(trajectory)\n",
        " \n",
        "  def result(self):\n",
        "    return self._metric_class.aggregate(self._metrics)\n",
        "```\n",
        "---\n",
        "\n",
        "Note that the different metrics for the items in the batch are combined using the `aggregate()` method. `aggregate()`  is defined in the base class `PyMetric` and specifies how metrics of a certain class are aggregated. The default behaviour is to average them.\n",
        "\n",
        "Also note that once a specific batch size has been used, all further calls to `BatchedPyMetric` must be done with `Trajectories` batched with the same size."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FAwZpX5a2cRM"
      },
      "source": [
        "The AverageReturnMetric can be called with batched time steps in the following way:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Ua6HwnfO2ccK"
      },
      "outputs": [],
      "source": [
        "# TODO(b/112359343): Update from time_steps to trajectories \n",
        "\n",
        "\n",
        "# STEP_TYPE = np.ones([2], dtype=np.int32)\n",
        "# DISCOUNT = np.ones([2], dtype=np.float32)\n",
        "# OBSERVATION = np.ones([2, 2])\n",
        "\n",
        "# ts0 = ts.TimeStep(step_type=0 * STEP_TYPE, discount=DISCOUNT, observation=OBSERVATION, reward=np.array([0., 0.]))\n",
        "# ts1 = ts.TimeStep(step_type=1 * STEP_TYPE, discount=DISCOUNT, observation=OBSERVATION, reward=np.array([1., 4.]))\n",
        "# ts2 = ts.TimeStep(step_type=1 * STEP_TYPE, discount=DISCOUNT, observation=OBSERVATION, reward=np.array([2., 5.]))\n",
        "# ts3 = ts.TimeStep(step_type=2 * STEP_TYPE, discount=DISCOUNT, observation=OBSERVATION, reward=np.array([3., 6.]))\n",
        "\n",
        "# batched_metric = batched_py_metric.BatchedPyMetric(\n",
        "#         py_metrics.AverageReturnMetric)\n",
        "\n",
        "# batched_metric(ts0)\n",
        "# batched_metric(ts1)\n",
        "# batched_metric(ts2)\n",
        "# batched_metric(ts3)\n",
        "\n",
        "# print(batched_metric.result())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Dw-kikMSqyFX"
      },
      "source": [
        "# TensorFlow Metric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DAJuPBJ_5ISB"
      },
      "source": [
        "TensorFlow metrics are usually used during data collection, e.g. to measure the average length of episodes, average return of the current policy etc.\n",
        "\n",
        "TF metrics are derived from TF Eager metrics and roughly follow the same interface as Python metrics. The main methods are:\n",
        "\n",
        "TODO(kbanoop): Should we make this interface and py_metric.Base closer? Main differences are no `reset()` and returning the arguments from `__call__()`.\n",
        "\n",
        "---\n",
        "```python\n",
        "class TFStepMetric(eager_metrics.Metric)\n",
        "\n",
        "  def __call__(self, *args, **kwargs):\n",
        "    \"\"\"Update the metric\"\"\"   \n",
        "    if not self._built:\n",
        "        self.build(*args, **kwargs)\n",
        "      self._built = True\n",
        "    return self.call(*args, **kwargs)  \n",
        "  \n",
        "  @abc.abstractmethod\n",
        "  def call(self, *args, **kwargs):\n",
        "    \n",
        "  @abc.abstractmethod    \n",
        "  def  build(self, *args, **kwargs):\n",
        "    \n",
        "  @abc.abstractmethod\n",
        "  def result(self):    \n",
        "\n",
        "```\n",
        "---\n",
        "\n",
        "\n",
        "There are a few key differences to keep in mind. Since all TensorFlow environments are batched by default, the `call()` method has to handle batches of time steps and actions. Also `call` has to return its input trajectory, so that metrics can be chained together, i.e.:\n",
        "\n",
        "```python\n",
        "for metric in metrics: \n",
        "  trajectory = metric(trajectory)\n",
        "  ```\n",
        "  \n",
        "Most metrics have internal tensorflow variables to keep track of the state of the metric. These are built lazily the first time the `call` method is called. The `build` method has to be overriden to construct these variables. Now let us look at a couple of examples (for more, see `tf_agents/metrics/tf_metrics.py`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9uilCOseRyVg"
      },
      "source": [
        "## Example 1: NumberOfEpisodes Metric\n",
        "\n",
        "The NumberOfEpisodes metric is used to count the number of episodes, e.g. while collecting data in TensorFlow. The implementation roughly looks like this: \n",
        "\n",
        "---\n",
        "```python\n",
        "class NumberOfEpisodes(tf_metric.TFStepMetric):\n",
        "  \"\"\"Counts the number of episodes in the environment.\"\"\"\n",
        "\n",
        "  def build(self, *args, **kwargs):\n",
        "    self.number_episodes = self.add_variable(shape=(), initializer=tf.zeros_initializer())\n",
        "\n",
        "  def call(self, trajectory):\n",
        "    num_episodes = tf.reduce_sum(trajectory.is_last())\n",
        "    self.number_episodes.assign_add(num_episodes)\n",
        "    # Dont we need a control dependency on the assign add?\n",
        "    return trajectory\n",
        "\n",
        "  def result(self):\n",
        "    return tf.identity(self.number_episodes)\n",
        "  ```\n",
        "---  \n",
        "The `call` method receives a batch of `Trajectories`.  `trajectory.is_last()` indicates if it was the last in an episode, so the sum of `trajectory.is_last()`  across all `trajectories` is a count of the number of episodes seen so far. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "TeP4FNoSCHdD"
      },
      "outputs": [],
      "source": [
        "# TODO(b/112359343): Update from time_steps to trajectories \n",
        "\n",
        "\n",
        "# with tf.Graph().as_default():\n",
        "#   ts0 = ts.restart(observation=tf.zeros(2, 2), batch_size=2)\n",
        "#   ts1 = ts.transition(observation=tf.zeros(2, 2), reward=tf.constant([1., 2.]))\n",
        "#   ts2 = ts.termination(observation=tf.zeros(2, 2), reward=tf.constant([3., 4.]))\n",
        "#   ts3 = ts.restart(observation=tf.zeros(2, 2), batch_size=2)\n",
        "#   ts4 = ts.transition(observation=tf.zeros(2, 2), reward=tf.constant([5., 6.]))\n",
        "#   ts5 = ts.termination(observation=tf.zeros(2, 2), reward=tf.constant([7., 8.]))\n",
        "  \n",
        "#   time_steps = [ts0, ts1, ts2, ts3, ts4, ts5]\n",
        "#   num_episodes = tf_metrics.NumberOfEpisodes()\n",
        "\n",
        "#   deps = []\n",
        "#   for i in range(len(time_steps)):\n",
        "#     with tf.control_dependencies(deps):\n",
        "#       time_step_action = num_episodes(time_steps[i])\n",
        "#       deps = nest.flatten(time_step_action)\n",
        "#   with tf.control_dependencies(deps):\n",
        "#     result = num_episodes.result()\n",
        "\n",
        "#   with tf.Session() as sess:\n",
        "#     sess.run(num_episodes.init_variables())\n",
        "#     print(sess.run(result))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uaWBpYuGq90e"
      },
      "source": [
        "## Example 2: TFPyMetric\n",
        "\n",
        "A `TFPyMetric` can be used to wrap any Python metric in TensorFlow. Metrics are usually easier to implement in Python compared to TensorFlow, however we might still want to use these Python metrics in a TensorFlow graph setting. The `TFPyMetric` allows us to do this very easily. Internally `TFPyMetric` wraps the methods of `PyMetric` using `py_func`'s. `TFPyMetric` can wrap both regular `PyMetrics` and `BatchedPyMetrics`. `TFPyMetric` is thread-safe.\n",
        "\n",
        "Let us look at an example. We will first wrap the `AverageReturnMetric` in a `BatchedPyMetric` so that it works with batches of Python `Trajectories`. Then we will wrap the `BatchedPyMetric` in a `TFPyMetric` so that it can work with batches of TensorFlow `Trajectories`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "FfOVwo9bHhyF"
      },
      "outputs": [],
      "source": [
        "# TODO(b/112359343): Update from time_steps to trajectories \n",
        "\n",
        "# with tf.Graph().as_default():\n",
        "#   ts0 = ts.restart(observation=tf.zeros(2, 2), batch_size=2)\n",
        "#   ts1 = ts.transition(observation=tf.zeros(2, 2), reward=tf.constant([1., 2.]))\n",
        "#   ts2 = ts.termination(observation=tf.zeros(2, 2), reward=tf.constant([3., 4.]))\n",
        "#   ts3 = ts.restart(observation=tf.zeros(2, 2), batch_size=2)\n",
        "#   ts4 = ts.transition(observation=tf.zeros(2, 2), reward=tf.constant([5., 6.]))\n",
        "#   ts5 = ts.termination(observation=tf.zeros(2, 2), reward=tf.constant([7., 8.]))\n",
        "  \n",
        "#   time_steps = [ts0, ts1, ts2, ts3, ts4, ts5]\n",
        "\n",
        "#   batched_avg_return_metric = batched_py_metric.BatchedPyMetric(\n",
        "#           py_metrics.AverageReturnMetric)    \n",
        "#   tf_avg_return_metric = tf_py_metric.TFPyMetric(batched_avg_return_metric)\n",
        "  \n",
        "#   deps = []\n",
        "#   for i in range(len(time_steps)):\n",
        "#     with tf.control_dependencies(deps):\n",
        "#       time_step_action = tf_avg_return_metric(time_steps[i])\n",
        "#       deps = nest.flatten(time_step_action)\n",
        "      \n",
        "#   with tf.control_dependencies(deps):\n",
        "#     result = tf_avg_return_metric.result()\n",
        "\n",
        "#   with tf.Session() as sess:  \n",
        "#     result_ = sess.run(result)\n",
        "#   print(result_)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Metrics Tutorial.ipynb",
      "provenance": [],
      "toc_visible": true,
      "version": "0.3.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
