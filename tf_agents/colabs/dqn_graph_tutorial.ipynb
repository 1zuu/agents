{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "z5b39qQZukFe"
      },
      "source": [
        "##### Copyright 2018 The TF-Agents Authors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xkE9rPft5KTR"
      },
      "source": [
        "### Get Started\n",
        "\u003ctable class=\"tfo-notebook-buttons\" align=\"left\"\u003e\n",
        "  \u003ctd\u003e\n",
        "    \u003ca target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/agents/blob/master/tf_agents/colabs/dqn_graph_tutorial.ipynb\"\u003e\u003cimg src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" /\u003eRun in Google Colab\u003c/a\u003e\n",
        "  \u003c/td\u003e\n",
        "  \u003ctd\u003e\n",
        "    \u003ca target=\"_blank\" href=\"https://github.com/tensorflow/agents/blob/master/tf_agents/colabs/dqn_graph_tutorial.ipynb\"\u003e\u003cimg src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" /\u003eView source on GitHub\u003c/a\u003e\n",
        "  \u003c/td\u003e\n",
        "\u003c/table\u003e\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "WGIeDH3Aupyp"
      },
      "outputs": [],
      "source": [
        "# Note: If you haven't installed tf-agents, gym, or matplotlib yet, run:\n",
        "!pip install tf-agents\n",
        "!pip install gym\n",
        "!pip install matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SVcZyCSaQUEe"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "F_BRVZpmQUEW"
      },
      "outputs": [],
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import functools\n",
        "import tensorflow as tf\n",
        "\n",
        "from tf_agents.agents.dqn import dqn_agent\n",
        "from tf_agents.agents.dqn import q_network\n",
        "from tf_agents.drivers import dynamic_step_driver\n",
        "from tf_agents.environments import suite_gym\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.metrics import metric_utils\n",
        "from tf_agents.metrics import py_metrics\n",
        "from tf_agents.metrics import tf_metrics\n",
        "from tf_agents.metrics import tf_py_metric\n",
        "from tf_agents.policies import py_tf_policy\n",
        "from tf_agents.policies import random_tf_policy\n",
        "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
        "\n",
        "# Clear any leftover state from previous colabs run.\n",
        "# (This is not necessary for normal programs.)\n",
        "tf.reset_default_graph()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-EBYDDl95GZ3"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HJL8n_SYJ-zz"
      },
      "source": [
        "This example shows how to train a DQN agent in Graph mode, where we first create a TensorFlow graph to hold all our operations and later execute them in a TF session. This is the most computationally efficient way of using TensorFlow, but can be a little hard to debug. Other ways of using TF-Agents, and TensorFlow in general, are eager mode (see dqn_eager_tutorial) and out of graph mode, where only the necessary components such as networks are in TensorFlow (see dqn_oog_tutorial). To get a general understanding of DQN, check out the [DQN paper](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf) or our Introduction to DQN colab.\n",
        "\n",
        "Now we will walk you through all the components in an RL pipeline for training and evaluating a DQN agent on the Cartpole Environment:\n",
        "\n",
        "- Environment\n",
        "- Agent\n",
        "- Network\n",
        "- Replay Buffers\n",
        "- Policies\n",
        "- Data Collection\n",
        "- Training\n",
        "- Evaluation\n",
        "- Visualization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Mf1wc0SlQUES"
      },
      "source": [
        "## Params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "XxS2aazFQUEG"
      },
      "outputs": [],
      "source": [
        "env_name = 'CartPole-v0'\n",
        "num_iterations = 20000  # @param\n",
        "\n",
        "# Params for collect\n",
        "initial_collect_steps = 1000  # @param\n",
        "collect_steps_per_iteration = 1 # @param\n",
        "epsilon_greedy = 0.1 # @param\n",
        "replay_buffer_capacity = 100000 # @param\n",
        "\n",
        "# Params for target update\n",
        "target_update_tau = 0.05 # @param\n",
        "target_update_period = 5 # @param\n",
        "\n",
        "# Params for train\n",
        "batch_size = 64 # @param\n",
        "learning_rate = 1e-3 # @param\n",
        "gamma = 0.99 # @param\n",
        "\n",
        "# Params for eval\n",
        "num_eval_episodes = 100 # @param\n",
        "eval_interval = 1000 # @param\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "A_qGQK8ycmwi"
      },
      "source": [
        "# Creating the Graph\n",
        "\n",
        "In graph mode, we first construct a graph to hold all the operations and later execute them in a session. This is in contrast to eager mode, where the operations are executed immediately when they are defined."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VMsJC3DEgI0x"
      },
      "source": [
        "## Environment\n",
        "\n",
        "Environments in RL represent the task or problem that we are trying to solve. In TF-Agents, environments can be written in TensorFlow or pure python. The most important method in environments is the `time_step = environment.step(action)` method which performs one simulation step of the environment given an action and returns a `TimeStep` named tuple containing the next observation, reward, etc. Other important methods are `time_step_spec()` and `action_spec()` which return the specifications (types, shapes, bounds) of the `time_step` and `action` respectively. \n",
        "\n",
        "Standard environments can be easily created in TF-Agents using `suites`. We have different suites for loading environments from OpenAI Gym, Atari, DM Control etc given a string environment name. See the [environments tutorial](environments_tutorial.ipynb) for a detailed tutorial on environments in TF-Agents.\n",
        "\n",
        "In this example, we will create two environments: one for training and one for evaluation. The environment for evaluation will be in python. For efficiency, we will keep the training environment in the TensorFlow graph because the networks are in TensorFlow. This is done automatically using a wrapper called `TFPyEnvironment` (under the hood, this converts the python functions into TensorFlow ops).\n",
        "\n",
        "TODO: Consider evaling in TF as well, now that we have better metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "fa5wSXCPc3bj"
      },
      "outputs": [],
      "source": [
        "eval_py_env = suite_gym.load(env_name)\n",
        "train_tf_env = tf_py_environment.TFPyEnvironment(suite_gym.load(env_name))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "M62fHsedeDjM"
      },
      "source": [
        "## Network\n",
        "\n",
        "The DQN agent needs a network that can compute the Q values for every possible action given an observation (referred to as the \"state\" in the literature) as input. In this example we will use a set of fully connected neural network layers (Dense layers in Keras parlance).\n",
        "\n",
        "In TF-Agents, networks are built using Keras by subclassing the [`tf_agents.networks.network.Network`](https://github.com/tensorflow/agents/tree/master/tf_agents/networks/network.py) base class. A `Network` just needs to implement a `call` function that takes in a batch of observations, their associated `step_type`s (e.g. first, middle, last), and (optionally) a internal recurrent state. The agents/dqn/q_network.py file contains a standard implentation for DQN that supports convolutional and fully connected layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mc0VD2eWedhS"
      },
      "source": [
        "## Agent\n",
        "\n",
        "The main parameters required to create the DQN agent are:\n",
        "\n",
        "*   `time_step_spec`: Specs describing time steps produced by the environment.\n",
        "*   `action_spec`: Specs describing the actions expected by the environment.\n",
        "*  `q_network`: An instance of `network.Network` that takes in observations, step types, and (optionally) RNN state.\n",
        "*   `optimizer`: The optimizer for training the Q network.\n",
        "*   `epsilon_greedy`: Epsilon (probability of choosing a random action) of the Epsilon-Greedy policy used for data collection.\n",
        "*   `target_update_tau`: A factor for soft variable update of the target network. If set to 1, the weights are exactly copied from the Q network every period.\n",
        "*  `target_update_period`: The frequency at which the target networks are updated.\n",
        "*   `gamma`: The factor for discounting future rewards relative to immediate rewards.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "sFancYQQecqC"
      },
      "outputs": [],
      "source": [
        "global_step = tf.train.get_or_create_global_step()\n",
        "tf_agent = dqn_agent.DqnAgent(\n",
        "    train_tf_env.time_step_spec(),\n",
        "    train_tf_env.action_spec(),\n",
        "    q_network.QNetwork(\n",
        "        train_tf_env.time_step_spec().observation,\n",
        "        train_tf_env.action_spec()),\n",
        "    optimizer=tf.train.AdamOptimizer(learning_rate=learning_rate),\n",
        "    epsilon_greedy=epsilon_greedy,\n",
        "    target_update_tau=target_update_tau,\n",
        "    target_update_period=target_update_period,\n",
        "    gamma=gamma,\n",
        "    train_step_counter=global_step)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NLva6g2jdWgr"
      },
      "source": [
        "## Replay Buffer\n",
        "\n",
        "TODO: link to replay buffer tutorial once that is done.\n",
        "\n",
        "In order to keep track of the collected data and feed samples to our agent, we will use the TFUniformReplayBuffer provided in TF-Agents. This replay buffer stores its data in a TensorFlow variable (rather than in, e.g., a Python data structure) which makes it faster to access.\n",
        "\n",
        "This replay buffer is constructed by giving it a nest of specs describing the tensors that are to be stored. In most agents, including DQN, the agent stores trajectories. We can extract the spec through `tf_agent.collect_data_spec`.\n",
        "\n",
        "In order to sample data from the replay buffer we will create a `tf.data` pipeline with `replay_buffer.as_dataset()` which we can then feed to the agent's `train` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "n_eFxThReaMC"
      },
      "outputs": [],
      "source": [
        "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
        "    tf_agent.collect_data_spec,\n",
        "    batch_size=1,\n",
        "    max_length=replay_buffer_capacity)\n",
        "\n",
        "dataset = replay_buffer.as_dataset(\n",
        "    num_parallel_calls=3,\n",
        "    sample_batch_size=batch_size,\n",
        "    num_steps=2).prefetch(3)\n",
        "\n",
        "iterator = dataset.make_initializable_iterator()\n",
        "trajectories, unused_info = iterator.get_next()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PBAOOAYMe3zK"
      },
      "source": [
        "## Policies\n",
        "\n",
        "In TF-Agents, policies represent the standard notion of policies in RL: given a `time_step` produce an action or a distribution over actions. The main method is `policy_step = policy.step(time_step)` where `policy_step` is a named tuple `PolicyStep(action, state, info)`.  `policy_step.action` is the `action` to be applied to the environment, `state` represents the state for stateful (RNN) policies and `info` may contain auxiliary information such as log probabilities of the actions. For a more detailed description of policies, please see [the policies tutorial](https://github.com/tensorflow/agents/tree/master/tf_agents/colabs/policies_tutorial.ipynb).\n",
        "\n",
        "Agents expose two policies: the main policy that is used for evaluation/deployment (`agent.policy()`) and another (exploratory) policy that is used for data collection (`agent.collect_policy()`). Since the evaluation environment is in python, we wrap the eval policy in python using the `PyTFPolicy` wrapper. TODO: Consider switching eval to TF. \n",
        "\n",
        "Also, initially it is advantageous to collect some data using a purely random policy to get a good coverage of the state/action space, so we will create that as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "ZvSNRZpae7rK"
      },
      "outputs": [],
      "source": [
        "eval_policy = py_tf_policy.PyTFPolicy(tf_agent.policy)\n",
        "collect_policy = tf_agent.collect_policy\n",
        "initial_collect_policy = random_tf_policy.RandomTFPolicy(\n",
        "    train_tf_env.time_step_spec(), train_tf_env.action_spec())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "t0bzTWE5fISL"
      },
      "source": [
        "## Data Collection\n",
        "\n",
        "Data Collection is done using drivers, which is just a name for a a loop that runs a policy in an environment and broadcasts each time_step and action to a list of observers. We have drivers that run for a specific number of steps or episodes (see [the drivers tutorial](https://github.com/tensorflow/agents/tree/master/tf_agents/colabs/drivers_tutorial.ipynb)).\n",
        "\n",
        "Observers are defined as a callable that takes `Trajectory` data and processes it in some way. In this case we will define an observer to add data to the replay buffer. In general, observers can also include other things such as metrics that summarize performance over time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "NGVmwc6CfHj7"
      },
      "outputs": [],
      "source": [
        "observers = [replay_buffer.add_batch]\n",
        "\n",
        "initial_collect_op = dynamic_step_driver.DynamicStepDriver(\n",
        "    train_tf_env,\n",
        "    initial_collect_policy,\n",
        "    observers=observers,\n",
        "    num_steps=initial_collect_steps).run()\n",
        "\n",
        "collect_op = dynamic_step_driver.DynamicStepDriver(\n",
        "    train_tf_env,\n",
        "    collect_policy,\n",
        "    observers=observers,\n",
        "    num_steps=collect_steps_per_iteration).run()  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "axq0ryYYfzZ7"
      },
      "source": [
        "## Training\n",
        "\n",
        "Now we create a global step variable to keep track of how many times the network is updated and create a train op which does one step of training/network update. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "yzj01qDSfy6z"
      },
      "outputs": [],
      "source": [
        "experience, unused_info = iterator.get_next()\n",
        "train_op = tf_agent.train(experience)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6LjDtkvRf7Ji"
      },
      "source": [
        "The train() function looks roughly as\n",
        "follows:\n",
        "\n",
        "```python\n",
        "time_steps, actions, next_time_steps = self._experience_to_transitions(experience)\n",
        "loss = self.loss(time_steps, actions, next_time_steps)\n",
        "train_step = create_train_step(loss, optimizer, global_step=train_step_counter)\n",
        "\n",
        "with tf.control_dependencies([train_step]):\n",
        "  target_update_op = self.update_targets(\n",
        "      self._target_update_tau, self._target_update_period)\n",
        "\n",
        "with tf.control_dependencies([target_update_op]):\n",
        "  train_step = tf.identity(train_step)\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fEdKy78H-YZU"
      },
      "source": [
        "## Initialization\n",
        "\n",
        "We create the initialization ops at the end, because `tf.local/global_variables_initializer()` creates initializers only for the variables that exist in the graph. Additionally `agent.initialize()` creates ops for initializing the agent. In DQN, this is just an op that copies the weights from the Q network to the target Q network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "y2AVfmQrf9kr"
      },
      "outputs": [],
      "source": [
        "init_local_variables_op = tf.local_variables_initializer()\n",
        "init_global_variables_op = tf.global_variables_initializer()\n",
        "init_agent_op = tf_agent.initialize()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "E3JVsFkX9gFn"
      },
      "source": [
        "# Executing the graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vcBdUS_0hL5Q"
      },
      "source": [
        "To execute the graph, we have to create a session:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "uGLi4qYjhPa6"
      },
      "outputs": [],
      "source": [
        "sess = tf.InteractiveSession()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tJBKpnb4hbOq"
      },
      "source": [
        "## Initialization\n",
        "\n",
        "First we execute the initialzation ops. In addition to the variable and agent initializatio ops, we also run the initial_collect_op to fill the agent's replay buffer with some initial data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "nSu7OZGIhhQ6"
      },
      "outputs": [],
      "source": [
        "#@test {\"skip\": true}\n",
        "sess.run(init_global_variables_op)\n",
        "sess.run(init_local_variables_op)\n",
        "sess.run(init_agent_op)\n",
        "sess.run(initial_collect_op)\n",
        "sess.run(iterator.initializer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MUdAtxk1htLT"
      },
      "source": [
        "Before we start training, let us evaluate the current policy held by the agent. In RL, the most common metric is the `AverageReturnMetric`. The Return is the sum of rewards in an episode, and average return refers to averaging this across multiple episodes.\n",
        "\n",
        "The `metric_utils.compute()` method can be used to compute a list of metrics given a python environment and a python policy. The `num_episodes` argument can be used to set how many episodes we compute the average over.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "R85_D0DOhtkh"
      },
      "outputs": [],
      "source": [
        "#@test {\"skip\": true}\n",
        "average_return_metric = py_metrics.AverageReturnMetric()\n",
        "returns = []\n",
        "\n",
        "# Compute evaluation metrics.\n",
        "metric_utils.compute(\n",
        "    [average_return_metric],\n",
        "    eval_py_env,\n",
        "    eval_policy,\n",
        "    num_episodes=num_eval_episodes,\n",
        ")\n",
        "returns.append(average_return_metric.result())\n",
        "print('Step = {0}: Return = {1}'.format(0, returns[-1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RgiBED1FhqEz"
      },
      "source": [
        "## Main Train/Collect/Eval Loop\n",
        "\n",
        "The main loop main involves executing the collect and train ops, and also evaluating the policy at regular intervals, logging etc. Note in particular that the train and collect ops can be run in parallel easily using a single `session.run()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "1bS8lr9cgJTA"
      },
      "outputs": [],
      "source": [
        "#@test {\"skip\": true}\n",
        "# sess.make_callable is just a way to make sess.run faster when calling it repeatedly. \n",
        "train_step_call = sess.make_callable([train_op, global_step, collect_op])\n",
        "\n",
        "for _ in range(num_iterations):\n",
        "  # Train/collect/eval.\n",
        "  total_loss, global_step_val, _ = train_step_call()\n",
        "\n",
        "  if global_step_val % eval_interval == 0:\n",
        "    metric_utils.compute(\n",
        "        [average_return_metric],\n",
        "        eval_py_env,\n",
        "        eval_policy,\n",
        "        num_episodes=num_eval_episodes,\n",
        "    )\n",
        "    returns.append(average_return_metric.result())\n",
        "    print('Step = {0}: Return = {1}'.format(global_step_val, returns[-1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5QwsSgo8iCZ1"
      },
      "source": [
        "Finally, close the session."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "nPxpvAKGiHXi"
      },
      "outputs": [],
      "source": [
        "sess.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "68jNcA_TiJDq"
      },
      "source": [
        "## Visualization\n",
        "\n",
        "### Plots\n",
        "\n",
        "We can plot rewards vs global steps to see the performance of our agent. In `Cartpole-v0`, the environment gives a reward of +1 for every time step the pole stays up, and since the maximum number of steps is 200, the maximum possible reward is also 200."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "w6Nru3uwZ0lf"
      },
      "outputs": [],
      "source": [
        "#@test {\"skip\": true}\n",
        "plt.plot(range(0, num_iterations + 1, eval_interval), returns)\n",
        "plt.ylabel('Average Return')\n",
        "plt.xlabel('Global Step')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "M7-XpPP99Cy7"
      },
      "source": [
        "### Videos\n",
        "\n",
        "TODO: Use moviepy once pillow has been imported: b/63250444"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "last_runtime": {
        "build_target": "",
        "kind": "local"
      },
      "name": "DQN_In_Graph_Tutorial.ipynb",
      "provenance": [],
      "toc_visible": true,
      "version": "0.3.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
