{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "q_EyQ_Wt4tTn"
      },
      "source": [
        "# Set up\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xkE9rPft5KTR"
      },
      "source": [
        "## Launch local runtime\n",
        "\n",
        "To run this colab, you'll need to run your own Jupyter runtime in a Python environment with tf_agents installed. See instructions [here](https://research.google.com/colaboratory/local-runtimes.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "T1BAYxnL5CHG"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "9mbvii8xgIOp"
      },
      "outputs": [],
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import functools\n",
        "import tensorflow as tf\n",
        "\n",
        "from tf_agents.agents.dqn import dqn_agent\n",
        "from tf_agents.drivers import dynamic_step_driver\n",
        "from tf_agents.environments import suite_gym\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.metrics import metric_utils\n",
        "from tf_agents.metrics import py_metrics\n",
        "from tf_agents.metrics import tf_metrics\n",
        "from tf_agents.metrics import tf_py_metric\n",
        "from tf_agents.networks import q_network\n",
        "from tf_agents.policies import py_tf_policy\n",
        "from tf_agents.policies import random_tf_policy\n",
        "from tf_agents.replay_buffers import batched_replay_buffer\n",
        "\n",
        "nest = tf.contrib.framework.nest\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UVGvbi-B5ETP"
      },
      "source": [
        "## Params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "s5tbjkD-gioa"
      },
      "outputs": [],
      "source": [
        "env_name = 'CartPole-v0'\n",
        "num_iterations = 20000  # @param\n",
        "\n",
        "# Params for collect\n",
        "initial_collect_steps = 1000  # @param\n",
        "collect_steps_per_iteration = 1 # @param\n",
        "epsilon_greedy = 0.1 # @param\n",
        "replay_buffer_capacity = 100000 # @param\n",
        "\n",
        "# Params for target update\n",
        "target_update_tau = 0.05 # @param\n",
        "target_update_period = 5 # @param\n",
        "\n",
        "# Params for train\n",
        "batch_size = 64 # @param\n",
        "learning_rate = 1e-3 # @param\n",
        "gamma = 0.99 # @param\n",
        "\n",
        "# Params for eval\n",
        "num_eval_episodes = 100 # @param\n",
        "eval_interval = 1000 # @param\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-EBYDDl95GZ3"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HJL8n_SYJ-zz"
      },
      "source": [
        "This example shows how to train a DQN agent in Graph mode, where we first create a TensorFlow graph to hold all our operations and later execute them in a TF session. This is the most computationally efficient way of using TensorFlow, but can be a little hard to debug. Other ways of using TF-Agents, and TensorFlow in general, are eager mode (see dqn_eager_tutorial) and out of graph mode, where only the necessary components such as networks are in TensorFlow (see dqn_oog_tutorial). To get a general understanding of DQN, check out the [DQN paper](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf) or our Introduction to DQN colab.\n",
        "\n",
        "Now we will walk you through all the components in an RL pipeline for training and evaluating a DQN agent on the Cartpole Environment:\n",
        "\n",
        "- Environment\n",
        "- Agent\n",
        "- Network\n",
        "- Replay Buffers\n",
        "- Policies\n",
        "- Data Collection\n",
        "- Training\n",
        "- Evaluation\n",
        "- Visualization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "A_qGQK8ycmwi"
      },
      "source": [
        "# Creating the Graph\n",
        "\n",
        "In graph mode, we first construct a graph to hold all the operations and later execute them in a session. This is in contrast to eager mode, where the operations are executed immediately when they are defined."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VMsJC3DEgI0x"
      },
      "source": [
        "## Environment\n",
        "\n",
        "Environments in RL represent the task or problem that we are trying to solve. In TF-Agents, environments can be written in TensorFlow or pure python. The most important method in environments is the `time_step = environment.step(action)` method which performs one simulation step of the environment given an action and returns a `TimeStep` named tuple containing the next observation, reward, etc. Other important methods are `time_step_spec()` and `action_spec()` which return the specifications (types, shapes, bounds) of the `time_step` and `action` respectively. \n",
        "\n",
        "Standard environments can be easily created in TF-Agents using `suites`. We have different suites for loading environments from OpenAI Gym, Atari, DM Control etc given a string environment name. See the [environments tutorial](environments_tutorial.ipynb) for a detailed tutorial on environments in TF-Agents.\n",
        "\n",
        "In this example, we will create two environments: one for training and one for evaluation. The environment for evaluation will be in python. For efficiency, we will keep the training environment in the TensorFlow graph because the networks are in TensorFlow. This is done automatically using a wrapper called `TFPyEnvironment` (under the hood, this converts the python functions into TensorFlow ops).\n",
        "\n",
        "TODO: Consider evaling in TF as well, now that we have better metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "fa5wSXCPc3bj"
      },
      "outputs": [],
      "source": [
        "eval_py_env = suite_gym.load(env_name)\n",
        "train_tf_env = tf_py_environment.TFPyEnvironment(suite_gym.load(env_name))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "M62fHsedeDjM"
      },
      "source": [
        "## Network\n",
        "\n",
        "The DQN agent needs a network that can compute the Q values for each action. In this example we will use a set of fully connected layers.\n",
        "\n",
        "\n",
        " #TODO(oars): Update once we pass in a network instance to the agent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mc0VD2eWedhS"
      },
      "source": [
        "## Agent\n",
        "\n",
        "The main parameters required to create the DQN agent are:\n",
        "\n",
        "*   `time_step_spec`: Specs describing time steps produced by the environment.\n",
        "*   `action_spec`: Specs describing the actions expected by the environment.\n",
        "*  `q_network_ctor`: A function to create a Q network that predicts Q values for each action given a time_step.\n",
        "*   `optimizer`: The optimizer for training the Q network.\n",
        "*   `epsilon_greedy`: Epsilon (probability of choosing a random action) of the Epsilon-Greedy policy used for data collection.\n",
        "*   `target_update_tau`: A factor for soft variable update of the target network. If set to 1, the weights are exactly copied from the Q network.\n",
        "*  `target_update_period`: The frequency at which the target networks are updated.\n",
        "*   `gamma`: A factor for discounting future rewards relative to immediate rewards.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "sFancYQQecqC"
      },
      "outputs": [],
      "source": [
        "tf_agent = dqn_agent.DqnAgent(\n",
        "    train_tf_env.time_step_spec(),\n",
        "    train_tf_env.action_spec(),\n",
        "    q_network.QNetwork(\n",
        "        train_tf_env.time_step_spec().observation,\n",
        "        train_tf_env.action_spec()),\n",
        "    optimizer=tf.train.AdamOptimizer(learning_rate=learning_rate),\n",
        "    epsilon_greedy=epsilon_greedy,\n",
        "    target_update_tau=target_update_tau,\n",
        "    target_update_period=target_update_period,\n",
        "    gamma=gamma)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NLva6g2jdWgr"
      },
      "source": [
        "## Replay Buffer\n",
        "\n",
        "TODO: Rewrite this section after we move the replay buffer out of the agent.\n",
        "\n",
        "TODO: link to replay buffer tutorial once that is done.\n",
        "\n",
        "In order to keep track of the collected data we will use the BatchedUniformReplayBuffer provided in TF-Agents. \n",
        "\n",
        "This replay buffer is constructed by giving it a nest of specs describing the tensors that are to be stored. e.g. in DQN, the agent stores trajectories. We can extract the spec through `tf_agent.collect_data_spec()`.\n",
        "\n",
        "In order to sample data from the replay buffer we will create a `tf.data` pipeline which we can then feed to the agent's train method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "n_eFxThReaMC"
      },
      "outputs": [],
      "source": [
        "replay_buffer = batched_replay_buffer.BatchedReplayBuffer(\n",
        "    tf_agent.collect_data_spec(),\n",
        "    batch_size=1,\n",
        "    max_length=replay_buffer_capacity)\n",
        "\n",
        "dataset = replay_buffer.as_dataset(\n",
        "    num_parallel_calls=3,\n",
        "    sample_batch_size=batch_size,\n",
        "    num_steps=2,\n",
        "    time_stacked=True).prefetch(3)\n",
        "\n",
        "iterator = dataset.make_initializable_iterator()\n",
        "trajectories, unused_ids, unused_probs = iterator.get_next()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PBAOOAYMe3zK"
      },
      "source": [
        "## Policies\n",
        "\n",
        "In TF-Agents, policies represent the standard notion of policies in RL: given a `time_step` produce an action or a distribution over actions. The main method is `policy_step = policy.step(time_step)` where `policy_step` is a named tuple `PolicyStep(action, state, info)`.  `policy_step.action` is the `action` to be applied to the environment, `state` represents the state for stateful (RNN) policies and `info` may contain auxiliary information such as log probabilities of the actions. For a more detailed description of policies, please see XXX.\n",
        "\n",
        "Agents expose two policies: the main policy that is used for evaluation/deployment (agent.policy()) and another (exploratory) policy that is used for data collection (agent.collect_policy()). Since the evaluation environment is in python, we wrap the eval policy in python using the PyTFPolicy wrapper. TODO: Consider switching eval to TF. \n",
        "\n",
        "Also initially it is advantageous to collect some data using a purely random policy to get a good coverage of the state/action space, so we will create that as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "ZvSNRZpae7rK"
      },
      "outputs": [],
      "source": [
        "eval_policy = py_tf_policy.PyTFPolicy(tf_agent.policy())\n",
        "collect_policy = tf_agent.collect_policy()\n",
        "initial_collect_policy = random_tf_policy.RandomTFPolicy(\n",
        "    train_tf_env.time_step_spec(), train_tf_env.action_spec())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "t0bzTWE5fISL"
      },
      "source": [
        "## Data Collection\n",
        "\n",
        "Data Collection is done using drivers, which is just a name for a a loop that runs a policy in an environment and broadcasts each time_step and action to a list of observers. We have drivers that run for a specific number of steps or episodes. (see drivers_tutorial XXX).\n",
        "\n",
        "Observers are defined as a callable that takes `Trajectory` data and uses it. In this case we will define an observer to add data to the replay buffer. In general, observers can also include other things such as metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "NGVmwc6CfHj7"
      },
      "outputs": [],
      "source": [
        "observers = [replay_buffer.add_batch]\n",
        "\n",
        "initial_collect_op = dynamic_step_driver.DynamicStepDriver(\n",
        "    train_tf_env,\n",
        "    initial_collect_policy,\n",
        "    observers=observers,\n",
        "    num_steps=initial_collect_steps).run()\n",
        "\n",
        "collect_op = dynamic_step_driver.DynamicStepDriver(\n",
        "    train_tf_env,\n",
        "    collect_policy,\n",
        "    observers=observers,\n",
        "    num_steps=collect_steps_per_iteration).run()  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "axq0ryYYfzZ7"
      },
      "source": [
        "## Training\n",
        "\n",
        "Now we create a global step variable to keep track of how many times the network is updated and create a train op which does one step of training/network update. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "yzj01qDSfy6z"
      },
      "outputs": [],
      "source": [
        "global_step = tf.train.get_or_create_global_step()\n",
        "experience, unused_ids, unused_probs = iterator.get_next()\n",
        "train_op = tf_agent.train(experience, train_step_counter=global_step)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6LjDtkvRf7Ji"
      },
      "source": [
        "The train() function looks roughly as\n",
        "follows:\n",
        "\n",
        "```python\n",
        "batch, _, _ = iterator.get_next()\n",
        "time_steps, actions, next_time_steps = batch\n",
        "loss = self.loss(time_steps, actions, next_time_steps)\n",
        "train_step = create_train_step(loss, optimizer, global_step=train_step_counter)\n",
        "\n",
        "# Do we need these two dependencies? Couldn't we switch them around?\n",
        "with tf.control_dependencies([train_step]):\n",
        "   target_update_op = self.update_targets(\n",
        "      self._target_update_tau, self._target_update_period)\n",
        "\n",
        "with tf.control_dependencies([target_update_op]):\n",
        "  train_step = tf.identity(train_step)\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fEdKy78H-YZU"
      },
      "source": [
        "## Initialization\n",
        "\n",
        "We create the initialization ops at the end, because `tf.local/global_variables_initializer()` creates initializers only for the variables that exist in the graph. Additionally `agent.initialize()` creates ops for initializing the agent. In DQN, this is just an op that copies the weights from the Q network to the target Q network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "y2AVfmQrf9kr"
      },
      "outputs": [],
      "source": [
        "init_local_variables_op = tf.local_variables_initializer()\n",
        "init_global_variables_op = tf.global_variables_initializer()\n",
        "init_agent_op = tf_agent.initialize()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "E3JVsFkX9gFn"
      },
      "source": [
        "# Executing the graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vcBdUS_0hL5Q"
      },
      "source": [
        "To execute the graph, we have to create a session:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "uGLi4qYjhPa6"
      },
      "outputs": [],
      "source": [
        "sess = tf.InteractiveSession()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tJBKpnb4hbOq"
      },
      "source": [
        "## Initialization\n",
        "\n",
        "First we execute the initialzation ops. In addition to the variable and agent initializatio ops, we also run the initial_collect_op to fill the agent's replay buffer with some initial data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "nSu7OZGIhhQ6"
      },
      "outputs": [],
      "source": [
        "#@test {\"skip\": true}\n",
        "sess.run(init_global_variables_op)\n",
        "sess.run(init_local_variables_op)\n",
        "sess.run(init_agent_op)\n",
        "sess.run(initial_collect_op)\n",
        "sess.run(iterator.initializer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MUdAtxk1htLT"
      },
      "source": [
        "Before we start training, let us evaluate the current policy held by the agent. In RL, the most common metric is the `AverageReturnMetric`. The Return is the sum of rewards in an episode, and average return refers to averaging this across multiple episodes.\n",
        "\n",
        "The `metric_utils.compute()` method can be used to compute a list of metrics given a python environment and a python policy. The `num_episodes` argument can be used to set how many episodes we compute the average over.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "R85_D0DOhtkh"
      },
      "outputs": [],
      "source": [
        "#@test {\"skip\": true}\n",
        "average_return_metric = py_metrics.AverageReturnMetric()\n",
        "returns = []\n",
        "\n",
        "# Compute evaluation metrics.\n",
        "metric_utils.compute(\n",
        "    [average_return_metric],\n",
        "    eval_py_env,\n",
        "    eval_policy,\n",
        "    num_episodes=num_eval_episodes,\n",
        ")\n",
        "returns.append(average_return_metric.result())\n",
        "print('Step = {0}: Return = {1}'.format(0, returns[-1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RgiBED1FhqEz"
      },
      "source": [
        "## Main Train/Collect/Eval Loop\n",
        "\n",
        "The main loop main involves executing the collect and train ops, and also evaluating the policy at regular intervals, logging etc. Note in particular that the train and collect ops can be run in parallel easily using a single `session.run()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "1bS8lr9cgJTA"
      },
      "outputs": [],
      "source": [
        "#@test {\"skip\": true}\n",
        "# sess.make_callable is just a way to make sess.run faster when calling it repeatedly. \n",
        "train_step_call = sess.make_callable([train_op, global_step, collect_op])\n",
        "\n",
        "for _ in range(num_iterations):\n",
        "  # Train/collect/eval.\n",
        "  total_loss, global_step_val, _ = train_step_call()\n",
        "\n",
        "  if global_step_val % eval_interval == 0:\n",
        "    metric_utils.compute(\n",
        "        [average_return_metric],\n",
        "        eval_py_env,\n",
        "        eval_policy,\n",
        "        num_episodes=num_eval_episodes,\n",
        "    )\n",
        "    returns.append(average_return_metric.result())\n",
        "    print('Step = {0}: Return = {1}'.format(global_step_val, returns[-1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5QwsSgo8iCZ1"
      },
      "source": [
        "Finally, close the session."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "nPxpvAKGiHXi"
      },
      "outputs": [],
      "source": [
        "sess.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "68jNcA_TiJDq"
      },
      "source": [
        "## Visualization\n",
        "\n",
        "### Plots\n",
        "\n",
        "We can plot rewards vs global steps to see the performance of our agent. In `Cartpole-v0`, the environment gives a reward of +1 for every time step the pole stays up, and since the maximum number of steps is 200, the maximum possible reward is also 200."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "w6Nru3uwZ0lf"
      },
      "outputs": [],
      "source": [
        "#@test {\"skip\": true}\n",
        "plt.plot(range(0, num_iterations + 1, eval_interval), returns)\n",
        "plt.ylabel('Average Return')\n",
        "plt.xlabel('Global Step')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "M7-XpPP99Cy7"
      },
      "source": [
        "### Videos\n",
        "\n",
        "TODO: Use moviepy once pillow has been imported: b/63250444"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "DQN_In_Graph_Tutorial.ipynb",
      "provenance": [],
      "toc_visible": true,
      "version": "0.3.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
